# + slideshow={"slide_type": "skip"}
import sys

sys.path.append("project/interface/")
sys.path.append("../project/interface/")
from plots import plot_function, plot_function3D

import minitorch
from minitorch import Scalar, ScalarFunction, Context
import math
import chalk
from drawing import draw_boxes
from IPython.display import SVG, display
chalk.set_svg_draw_height(300)

# + [markdown] slideshow={"slide_type": "slide"}
# Module 1.3 - Backprop
# =================================



# + [markdown] slideshow={"slide_type": "slide"}
# Functions
# -----------

# * Function :math:`f(x) = x \times 5`
#
# * $x$ is unwrapped (python number) and return is a number

# + slideshow={"slide_type": "x"}
class TimesFive(ScalarFunction):

    @staticmethod
    def forward(ctx, x):
        return x * 5



Multi-arg Functions
----------------------

Code ::

      class Mul(ScalarFunction):
          @staticmethod
          def forward(ctx, x, y):
              return x * y

.. image:: figs/Autograd/autograd2.png
           :align: center


Context
----------

Arguments to backward must be saved in context. ::

  class Square(ScalarFunction):
      @staticmethod
      def forward(ctx, x):
          ctx.save_for_backward(x)
          return x * x

      @staticmethod
      def backward(ctx, d_out):
          x = ctx.saved_values
          f_prime = 2 * x
          return f_prime * d_out

Picture
----------

.. image:: figs/Autograd/autograd2.png
           :align: center

.. image:: figs/Autograd/autograd3.png
           :align: center



Lecture Quiz
-------------

  `Quiz`


Outline
---------

.. revealjs_fragments::

   * Variables and Functions
   * Backward
   * Chain Rule


How do we get derivatives?
--------------------------

* Base case: compute derivatives for single functions

* Inductive case: define how to propagate a derivative


Chain Rule
===========


Python Details
------------------

* Use `apply` for the above Functions ::

   x = minitorch.Scalar(10.)
   z = TimesFive.apply(x)
   out = TimesFive.apply(z)

* Apply unwraps, calls, and wraps again

Chaining Boxes
---------------

Chaining ::

   x = minitorch.Scalar(10., name="x")
   g_x = G.apply(x)
   f_g_x = F.apply(g_x)


.. image:: figs/Autograd/chain1.png
           :align: center


Chain Rule
-----------

* Compute derivative from chain

.. math ::

    f'_x(g(x)) = g'(x) \times f'_{g(x)}(g(x))


.. image:: figs/Autograd/chain2.png
           :align: center


Chain Rule
-----------

.. math::

   \begin{eqnarray*}
   z &=& g(x) \\
   d_{out} &=& f'(z) \\
   f'_x(g(x)) &=& g'(x) \times d_{out} \\
   \end{eqnarray*}


.. image:: figs/Autograd/chain2.png
           :align: center

Example: Chain Rule
--------------------

Chaining

.. math::

  log(x)^2

.. image:: figs/Autograd/chain1.png
           :align: center

.. math::

   f(z) = z^2\\
   g(x) = log(x)\\

Example: Chain Rule
--------------------

.. math::

   f'(z) = 2z \times 1 \\
   g'(x) =  1 / x


.. image:: figs/Autograd/chain2.png
           :align: center


What is the combination?

.. math::

   f'_x(g(x))

Two Arguments: Chain
--------------------

.. math::

  f(g(x, y))


Two Arguments: Chain
--------------------
.. math::

   \begin{eqnarray*}
  f'_x(g(x, y)) &=& g_x'(x, y) \times f'_{g(x, y)}(g(x, y)) \\
  f'_y(g(x, y)) &=& g_y'(x, y) \times f'_{g(x, y)}(g(x, y))
  \end{eqnarray*}

.. image:: figs/Autograd/chain3.png
           :align: center



Two Arguments: Chain
---------------------

.. math::

   \begin{eqnarray*}
   z &=& g(x, y) \\
   d_{out} &=& f'(z) \\
   f'_x(g(x, y)) &=& g_x'(x, y) \times d_{out} \\
   f'_y(g(x, y)) &=& g_y'(x, y) \times d_{out}
   \end{eqnarray*}

.. image:: figs/Autograd/chain3.png
           :align: center


Example: Chain Rule
--------------------

Chaining

.. math::

  (x * y)^2

.. image:: figs/Autograd/chain3.png
           :align: center

.. math::

   f(z) = z^2\\
   g(x, y) = (x * y)\\


Example: Chain Rule
--------------------

.. math::

   f'(z) = 2z \times 1\\
   g'_x(x, y) = y \\
   g'_y(x, y) = x \\

What is the combination?

.. math::

   f'_x(g(x, y)) = 2 z y\\
   f'_y(g(x, y)) = 2 z x \\


Multivariable Chain
--------------------

.. math::

  f(g(x), h(x))


Multivariable Chain
--------------------

.. math::

   \begin{eqnarray*}
   z_1 &=& g(x) \\
   z_2 &=& h(x) \\
   f'_x(g(x), h(y)) &=& g'(x) \times f'_{z_1}(z_1, z_2) + h'(x) \times f'_{z_2}(z_1, z_2)
   \end{eqnarray*}



Coding Derivatives
--------------------

* For each :math:`f` or :math:`g`  we need to also provide :math:`f'` and :math:`g'`
* This part can be done through local symbolic or numeric differentiation

.. image:: figs/Autograd/autograd3.png
           :align: center


Picture
----------

.. image:: figs/Autograd/autograd3.png
           :align: center



Backpropagation
=================

Goal
------

* Efficient implementation of chain-rule

* Assume access to the graph.

* Goal: Call backward once per variable / d_out


Full Graph
-----------

.. math::

   \begin{eqnarray*}
   z &=& x \times y \\
   h(x, y) &=& \log(z) + \exp(z)
   \end{eqnarray*}


.. image:: figs/Autograd/backprop1.png
           :align: center

Tool
-------------

If we have:

*  the derivative with respect to a Variable
*  the Function that created the Variable

We can apply the chain rule through that function.

Step
-----------

.. image:: figs/Autograd/backprop3.png
           :align: center

.. image:: figs/Autograd/backprop4.png
           :align: center

Issue
----------

Order matters!

* If we proceed without finishing a variable,
  we may need to apply chain rule multiple times

Desired property: all derivatives for a variable before backward.

Ordering Step
--------------

* Do not process any Variable until all downstream Variables are done.

* Collect a list of the Variables first.


Topological Sorting
---------------------

* `Topological Sorting <http://wikipedia.org/wiki/Topological_sorting>`_

* High-level -> Run depth first search and mark nodes.


Topological Sorting
---------------------

Pseudocode ::

  visit(last)

  function visit(node n)
    if n has a mark then return

    for each node m with an edge from n to m do
        visit(m)

    mark n with a permanent mark
    add n to list


Backpropagation
----------------

.. revealjs_fragments::

   * Graph propagation
   * Ensure flow to original Variables.

Terminology
------------

.. revealjs_fragments::

   * Leaf: Variable created from scratch
   * Non-Leaf: Variable created with a Function
   * Constant: Term passed in that is not a variable

Algorithm: Outer Loop
-----------------------

.. revealjs_fragments::

 0. Call topological sort
 1. Create dict of Variables and derivatives
 2. For each node in backward order:

Algorithm: Inner Loop
----------------------

a. if Variable is leaf, add its final derivative
b. if the Variable is not a leaf,

      1) call backward with :math:`d_{out}`
      2) loop through all the Variables+derivative
      3) accumulate derivatives for the Variable


Example
-----------

.. image:: figs/Autograd/backprop1.png
           :align: center



Example
-----------

.. image:: figs/Autograd/backprop2.png
           :align: center

Example
-----------

.. image:: figs/Autograd/backprop3.png
           :align: center

Example
-----------

.. image:: figs/Autograd/backprop4.png
           :align: center

Example
-----------

.. image:: figs/Autograd/backprop5.png
           :align: center
Example
-----------

.. image:: figs/Autograd/backprop6.png
           :align: center
Example
-----------

.. image:: figs/Autograd/backprop7.png
           :align: center


Neural Networks
=================


Neural Networks
------------------

.. revealjs_fragments::

   * New *model*
   * Uses repeated linear splits of data
   * Produces non-linear separators
   * Loss will not change



Training
---------

.. code ::

   model = Network()
   ...
   model.named_parameters()

* All the parameters in model are leaf Variables
* Computing backward on loss fills their derivative



Math View
----------

.. math::

   \begin{eqnarray*}
   h_ 1 &=& \text{ReLU}(x_1 \times w^0_1 + x_2 \times w^0_2 + b^0) \\
   h_ 2 &=& \text{ReLU}(x_1 \times w^1_1 + x_2 \times w^1_2 + b^1)\\
   m(x_1, x_2) &=& h_1 \times w_1 + h_2 \times w_2 + b
   \end{eqnarray*}

Parameters:
 :math:`w_1, w_2, w^0_1, w^0_2, w^1_1, w^1_2, b, b^0, b^1`

Math View (Alt)
---------------

.. math::

   \begin{eqnarray*}
   \text{lin}(x; w, b) &=& x_1 \times w_1 + x_2 \times w_2 + b \\
   h_ 1 &=& \text{ReLU}(\text{lin}(x; w^0, b^0)) \\
   h_ 2 &=& \text{ReLU}(\text{lin}(x; w^1, b^1))\\
   m(x_1, x_2) &=& \text{lin}(h; w, b)
   \end{eqnarray*}

Parameters:
 :math:`w_1, w_2, w^0_1, w^0_2, w^1_1, w^1_2, b, b^0, b^1`

Code
-----

* Code in `run_scalar.py`


Code
-----

* Optim to move the parameters.

Q&A
=====

