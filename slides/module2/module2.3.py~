

# Module 2.3 - Gradients
# ========================



# Module 2.3 - Gradients
# -----------------------

#    Gradients


# Tensor Functions
# -----------------

# Unary ::

#     new_tensor = tensor.log()

# Binary (for now, only same shape) ::

#     new_tensor = tensor1 + tensor2

# Reductions ::

#     new_tensor = tensor.sum()


# Tensor Ops
# -----------------

# .. revealjs_fragments::

#    1) **Map** - Apply to all elements

#    2) **Zip** (same as zipWith) - Apply to all pairs

#    3) **Reduce** - Reduce a subset


# Map
# ---------

# .. image:: figs/Ops/map.png
#            :align: center


# Zip
# ---------

# .. image:: figs/Ops/zip.png
#            :align: center

# Reduce
# ---------

# .. image:: figs/Ops/reduce.png
#            :align: center


# Reduction
# ---------
# .. image:: figs/Reduce/sum0.png
#            :align: center

# Reduction
# ---------

# .. image:: figs/Reduce/sum1.png
#            :align: center

# Reduction
# ---------

# .. image:: figs/Reduce/implement.png
#            :align: center


# Reduction
# ---------

# .. image:: figs/Reduce/sum2.png
#            :align: center



# Outline
# --------

# .. revealjs_fragments::

#    * Gradients
#    * Challenges



# Broadcasting
# ==============

# High Level
# ---------------

# .. revealjs_fragments::

#    * Apply same operation multiple times
#    * Avoid loops and writes
#    * Save memory


# First Challenge
# ---------------

# .. revealjs_fragments::

#    * Relaxing Zip constraints
#    * Apply zip without shapes being identical


# Motivation: Scalar Addition
# ------------------------------

# .. math::

#    vector1 + 10

# Naive Scalar Addition 1
# -----------------------

# * Repeat vector-size

# .. math::

#    vector1 + [10, 10, 10]

# .. code::

#   vector1 + tensor([10, 10, 10])

# Naive Scalar Addition 2
# -------------------------

# * Write a `for` loop

# .. code::

#   temp_vector = zeros(vector1.shape)
#   for i in vector.shape[0]:
#       temp_vector[i] = vector1[i] + 10



# Broadcasting
# ------------

# .. revealjs_fragments::

#    * No intermediate terms
#    * Define rules to make different shapes work together
#    * Avoid for loops entirely


# Zip With Broadcasting
# ---------------------


# .. image:: figs/Ops/zip\ broad\ back.png

# Zip With Broadcasting
# ---------------------

# Code ::

#   out = zeros(3, 2)
#   for i in range(3):
#       for j in range(2):
#           out[i, j] = a[i] + b[j]

# Zip Broadcasting
# ---------------------

# .. image:: figs/Ops/zip\ broad.png


# Rules
# -----

# *   **Rule 1**: Dimension of size 1 broadcasts with anything
# *   **Rule 2**: Extra dimensions of 1 can be added with `view`
# *   **Rule 3**: Zip automatically adds starting dims of size 1


# Matrix Scalar Addition
# -----------------------

# Matrix + Scalar ::

#     matrix1 + tensor([10])

# Matrix Scalar Addition
# -----------------------

# Matrix + Vector ::

#     matrix1.view(4, 3) + tensor([1, 2, 3])

# Matrix Scalar Addition
# -----------------------

# Doesn't Work! ::

#     matrix1.view(4, 3) + tensor([1, 2, 3, 5])

# Does Work! ::

#     matrix1.view(4, 3) + tensor([1, 2, 3, 5]).view(4, 1)

# Applying the Rules
# -------------------


# * (3, 4, 5) | (3, 1, 5) => (3, 4, 5)
# * (3, 4, 1) | (3, 1, 5) => (3, 4, 5)
# * (3, 4, 1) | (1, 5) => (3, 4, 5)
# * (3, 4, 1) | (3, 5) => X

# Exercises
# -------------------

# * (1, 3, 4) | (1, 3, 1) => ?
# * (1, 4, 4) | (3, 1, 5) => ?
# * (3, 4, 1) | (1) => ?


# Examples
# =========

# Tensor-Scalar operations
# ------------------------

# .. image:: figs/Broadcast/scalar.png
#            :align: center

# Matrix-vector operations
# -------------------------


# .. image:: figs/Broadcast/vector.png
#            :align: center

# Matrix-matrix operations
# ---------------------------

# .. image:: figs/Broadcast/threed.png
#            :align: center

# Matrix-Matrix Operations
# ------------------------


# .. image:: figs/Broadcast/matmul.png
#            :width: 600px
#            :align: center




# Implementation
# ---------------

# * Never create an intermediate value.
# * Implicit map between output space / input space


# Functions
# ----------

# * `shape_broadcast` - create the broadcast dims

# * `broadcast_index` - map from broadcasted to original value



# Gradients
# ==========

# Derivatives
# ------------

# .. revealjs_fragments::

#    * Each tensor arg is many args
#    * Returning tensor is like running several Functions
#    * Result `backward` needs to run chain-rule for each arg and output.


# Graph
# ------


# Terminology
# ------------

# .. revealjs_fragments::

#    * Scalar -> Tensor
#    * Derivative -> Gradient
#    * `d_out` ->  `grad_out`
#    * Recommendation: Reason through gradients as many derivatives


# Gradient
# --------

# .. math::

#    F([x_1, x_2, ..., x_N])

# Note -> producing a scalar


# Gradient
# --------

# .. math::

#    F'_{x_1}([x_1, x_2, ..., x_N]) \\
#    F'_{x_2}([x_1, x_2, ..., x_N]) \\
#    ... \\
#    F'_{x_N}([x_1, x_2, ..., x_N]) \\


# Each is a standard derivative


# Gradient
# --------

# .. math::

#    [F'_{x_1}([x_1, x_2, ..., x_N]), \\
#     F'_{x_2}([x_1, x_2, ..., x_N]), \\
#     ... \\
#     F'_{x_N}([x_1, x_2, ..., x_N])]


# Tensor of derivatives.


# Chain Rule For Gradients
# ---------------------------

# .. math::

#   F(G(x))

# .. math::

#   F'_x(G(x)) = F'_{G(x)}(G(x)) \cdot G'(x)

# (Not Critical for us)


# Avoiding Gradient Math
# ---------------------------

# * All of this is just notation for scalars

# * Can always reason about it with scalars directly



# Map Gradient
# ------------

# .. image:: figs/Ops/map\ back.png
#            :align: center

# Zip Gradient
# ------------

# .. image:: figs/Ops/zip\ back.png
#            :align: center


# Reduce Gradient
# ----------------


# .. image:: figs/Ops/reduce\ back.png
#            :align: center



# Q&A
# ----


