
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Efficiency &#8212; MiniTorch 0.3 documentation</title>
    
    <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
  
    
    <link rel="stylesheet"
      href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
    <link rel="preload" as="font" type="font/woff2" crossorigin
      href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">
  
    
      
  
    
    <link rel="stylesheet" href="_static/styles/pydata-sphinx-theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    
    <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">
  
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Parallel Computation" href="parallel.html" />
    <link rel="prev" title="Tensor Variables" href="tensor.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="english">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/minitorch.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="setup.html">
  Setup
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="mlprimer.html">
  ML Primer
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module0.html">
  Fundamentals
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module1.html">
  Autodiff
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module2.html">
  Tensors
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Efficiency
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module4.html">
  Networks
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/minitorch/" rel="noopener" target="_blank" title="GitHub"><span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label></a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/srush_nlp" rel="noopener" target="_blank" title="Twitter"><span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label></a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p>
 <span class="caption-text">
  Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="parallel.html">
   Parallel Computation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="matrixmult.html">
   Fusing Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cuda.html">
   GPU Programming
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tasks">
   Tasks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-3-1-parallelization">
     Task 3.1: Parallelization
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-3-2-matrix-multiplication">
     Task 3.2: Matrix Multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-3-3-cuda-operations">
     Task 3.3: CUDA Operations
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-3-4-cuda-matrix-multiplication">
     Task 3.4: CUDA Matrix Multiplication
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-3-5-training">
     Task 3.5: Training
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="efficiency">
<h1>Efficiency<a class="headerlink" href="#efficiency" title="Permalink to this headline">¶</a></h1>
<img alt="_images/threadid&#64;3x.png" class="align-center" src="_images/threadid&#64;3x.png" />
<p>In addition to helping simplify code, tensors provide a basis for
speeding up computation. In fact, they are really the only way to
efficiently write deep learning code in a slow language like Python.
However, nothing we have done so far really makes anything faster than
<a class="reference internal" href="module0.html"><span class="doc">Fundamentals</span></a>. This module is focused on taking advantage of tensors
to write fast code, first on standard CPUs and then using GPUs.</p>
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-3">https://github.com/minitorch/Module-3</a> .</p>
<p>To begin, remember to activate your virtual environment first, and then
clone your assignment:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT3_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT_NAME</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Ue</span> <span class="o">.</span>
</pre></div>
</div>
<p>You need the files from previous assignments, so maker sure to pull them over
to your new repo.</p>
<p>Be sure to continue to follow the <a class="reference internal" href="contributing.html"><span class="doc">Contributing</span></a> guidelines.</p>
<div class="toctree-wrapper compound">
<p><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="parallel.html">Parallel Computation</a></li>
<li class="toctree-l1"><a class="reference internal" href="matrixmult.html">Fusing Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="cuda.html">GPU Programming</a></li>
</ul>
</div>
<section id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">¶</a></h2>
<p>For this assignment you will need to run you commands in the Google Colab virtual environment.
Follow these instructions for
<a class="reference external" href="https://colab.research.google.com/drive/1zVoQCaTlTU6bkuqMXQTakJxr7oFYdSV2?usp=sharing">Colab setup</a>.</p>
<section id="task-3-1-parallelization">
<h3>Task 3.1: Parallelization<a class="headerlink" href="#task-3-1-parallelization" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with Numba <cite>prange</cite>.
Be sure to very carefully read the section on
<a class="reference internal" href="parallel.html"><span class="doc">Parallel Computation</span></a>,
<a class="reference external" href="https://numba.pydata.org/numba-doc/latest/user/parallel.html">Numba</a>
and review <a class="reference internal" href="module2.html"><span class="doc">Tensors</span></a>.</p>
</div>
<p>The main backend for our codebase are the three functions <cite>map</cite>,
<cite>zip</cite>, and <cite>reduce</cite>. If we can speed up these three, everything we
built so far will get better. This exercise asks you to utilize Numba
and the <cite>njit</cite> function to speed up these functions. In particular if
you can utilize parallelization through <cite>prange</cite> you can get some big
wins. Be careful though! Parallelization can lead to funny bugs.</p>
<p>In order to help debug this code, we have created a parallel analytics
script for you</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">project</span><span class="o">/</span><span class="n">parallel_check</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Running this script will run NUMBA diagnostics on your functions.</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Complete the following in <cite>minitorch/fast_ops.py</cite> and pass tests
marked as <cite>task3_1</cite>.</p>
<ul class="simple">
<li><p>Include the diagnostics output from the above script in your README.</p></li>
<li><p>Be sure that the code implements the optimizations specified in the
docstrings. We will check for this explicitly.</p></li>
</ul>
</div>
<dl class="py function">
<dt id="minitorch.fast_ops.tensor_map">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>float<span class="p">]</span><span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#minitorch.fast_ops.tensor_map" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA low_level tensor_map function. See <cite>tensor_ops.py</cite> for description.</p>
<p>Optimizations:</p>
<blockquote>
<div><ul class="simple">
<li><p>Main loop in parallel</p></li>
<li><p>All indices use numpy buffers</p></li>
<li><p>When <cite>out</cite> and <cite>in</cite> are stride-aligned, avoid indexing</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings floats-to-floats to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for out tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for out tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for out tensor.</p></li>
<li><p><strong>in_storage</strong> (<em>array</em>) -- storage for in tensor.</p></li>
<li><p><strong>in_shape</strong> (<em>array</em>) -- shape for in tensor.</p></li>
<li><p><strong>in_strides</strong> (<em>array</em>) -- strides for in tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.fast_ops.tensor_zip">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_zip</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span><span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#minitorch.fast_ops.tensor_zip" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA higher-order tensor zip function. See <cite>tensor_ops.py</cite> for description.</p>
<p>Optimizations:</p>
<blockquote>
<div><ul class="simple">
<li><p>Main loop in parallel</p></li>
<li><p>All indices use numpy buffers</p></li>
<li><p>When <cite>out</cite>, <cite>a</cite>, <cite>b</cite> are stride-aligned, avoid indexing</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function maps two floats to float to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.fast_ops.tensor_reduce">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_reduce</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span><span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#minitorch.fast_ops.tensor_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA higher-order tensor reduce function. See <cite>tensor_ops.py</cite> for description.</p>
<p>Optimizations:</p>
<blockquote>
<div><ul class="simple">
<li><p>Main loop in parallel</p></li>
<li><p>All indices use numpy buffers</p></li>
<li><p>Inner-loop should not call any functions or write non-local variables</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- reduction function mapping two floats to float.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>reduce_dim</strong> (<em>int</em>) -- dimension to reduce out</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="task-3-2-matrix-multiplication">
<h3>Task 3.2: Matrix Multiplication<a class="headerlink" href="#task-3-2-matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with matrix multiplication.
Be sure to read the Guide on
<a class="reference internal" href="matrixmult.html"><span class="doc">Fusing Operations</span></a>.</p>
</div>
<p>Matrix multiplication is key to all of the models that we have trained
so far.  In the last module, we computed matrix multiplication using
broadcasting.  In this task, we ask you to implement it directly as a
function. Do your best to make the function efficient, but for now all
that matters is that you correctly produce a multiply function that
passes our tests and has some parallelism.</p>
<p>In order to use this function, you will also need to add a new
<cite>MatMul</cite> Function to <cite>tensor_functions.py</cite>. We have added a version in
the starter code you can copy.  You might also find it useful to add a
slow broadcasted <cite>matrix_multiply</cite> to <cite>tensor_ops.py</cite> for debugging.</p>
<p>In order to help debug this code, we have created a parallel analytics
script for you</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">project</span><span class="o">/</span><span class="n">parallel_test</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>Running this script will run NUMBA diagnostics on your functions.</p>
<p>After you finish this task, you may want to skip to 3.5 and experiment
with training on the real task under speed conditions.</p>
<div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/fast_ops.py</cite>.
Pass tests marked as <cite>task3_2</cite>.</p>
<ul class="simple">
<li><p>Include the diagnostics output from the above script in your README.</p></li>
<li><p>Be sure that the code implements the optimizations specified in the
docstrings. We will check for this explicitly.</p></li>
</ul>
</div>
<dl class="py function">
<dt id="minitorch.fast_ops.tensor_matrix_multiply">
<code class="sig-prename descclassname">minitorch.fast_ops.</code><code class="sig-name descname">tensor_matrix_multiply</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">Storage</span></em>, <em class="sig-param"><span class="n">out_shape</span><span class="p">:</span> <span class="n">Shape</span></em>, <em class="sig-param"><span class="n">out_strides</span><span class="p">:</span> <span class="n">Strides</span></em>, <em class="sig-param"><span class="n">a_storage</span><span class="p">:</span> <span class="n">Storage</span></em>, <em class="sig-param"><span class="n">a_shape</span><span class="p">:</span> <span class="n">Shape</span></em>, <em class="sig-param"><span class="n">a_strides</span><span class="p">:</span> <span class="n">Strides</span></em>, <em class="sig-param"><span class="n">b_storage</span><span class="p">:</span> <span class="n">Storage</span></em>, <em class="sig-param"><span class="n">b_shape</span><span class="p">:</span> <span class="n">Shape</span></em>, <em class="sig-param"><span class="n">b_strides</span><span class="p">:</span> <span class="n">Strides</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#minitorch.fast_ops.tensor_matrix_multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>NUMBA tensor matrix multiply function.</p>
<p>Should work for any tensor shapes that broadcast as long as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">a_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">b_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
</pre></div>
</div>
<p>Optimizations:</p>
<blockquote>
<div><ul class="simple">
<li><p>Outer loop in parallel</p></li>
<li><p>No index buffers or function calls</p></li>
<li><p>Inner loop should have no global writes, 1 multiply.</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="task-3-3-cuda-operations">
<h3>Task 3.3: CUDA Operations<a class="headerlink" href="#task-3-3-cuda-operations" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with CUDA.
Be sure to read the Guide on
<a class="reference internal" href="cuda.html"><span class="doc">GPU Programming</span></a> and the Numba CUDA guide.</p>
</div>
<p>We can do even better than parallelization if we have access to
specialized hardware. This task asks you to build a GPU implementation
of the backend operations. It will be hard to equal what PyTorch does, but
if you are clever you can make these computations really fast (aim for 2x
of task 3.1).</p>
<p>Reduce is a particularly challenging function. We provide guides and a simple
practice function to help you get started.</p>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/cuda_ops.py</cite>, and pass the tests marked as
<cite>task3_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_map">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>float<span class="p">]</span><span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#minitorch.cuda_ops.tensor_map" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA higher-order tensor map function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fn_map</span> <span class="o">=</span> <span class="n">tensor_map</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">fn_map</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">...</span> <span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings floats-to-floats to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for out tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for out tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for out tensor.</p></li>
<li><p><strong>out_size</strong> (<em>array</em>) -- size for out tensor.</p></li>
<li><p><strong>in_storage</strong> (<em>array</em>) -- storage for in tensor.</p></li>
<li><p><strong>in_shape</strong> (<em>array</em>) -- shape for in tensor.</p></li>
<li><p><strong>in_strides</strong> (<em>array</em>) -- strides for in tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_zip">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_zip</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span><span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#minitorch.cuda_ops.tensor_zip" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA higher-order tensor zipWith (or map2) function</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fn_zip</span> <span class="o">=</span> <span class="n">tensor_zip</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>
<span class="n">fn_zip</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings two floats to float to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>array</em>) -- size for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.cuda_ops._sum_practice">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">_sum_practice</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">numpy.ndarray<span class="p">[</span>Any<span class="p">, </span>numpy.dtype<span class="p">[</span>numpy.float64<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">a</span><span class="p">:</span> <span class="n">numpy.ndarray<span class="p">[</span>Any<span class="p">, </span>numpy.dtype<span class="p">[</span>numpy.float64<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">size</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#minitorch.cuda_ops._sum_practice" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a practice sum kernel to prepare for reduce.</p>
<p>Given an array of length <span class="math notranslate nohighlight">\(n\)</span> and out of size <span class="math notranslate nohighlight">\(n // blockDIM\)</span>
it should sum up each blockDim values into an out cell.</p>
<p>[a_1, a_2, ..., a_100]</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>[a_1 +...+ a_32, a_32 + ... + a_64, ... ,]</p>
<p>Note: Each block must do the sum using shared memory!</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>a</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>size</strong> (<em>int</em>) -- length of a.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_reduce">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_reduce</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>float<span class="p">, </span>float<span class="p">]</span><span class="p">, </span>float<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#minitorch.cuda_ops.tensor_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA higher-order tensor reduce function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- reduction function maps two floats to float.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>array</em>) -- size for <cite>out</cite> tensor.</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>reduce_dim</strong> (<em>int</em>) -- dimension to reduce out</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fills in <cite>out</cite></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>None</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="task-3-4-cuda-matrix-multiplication">
<h3>Task 3.4: CUDA Matrix Multiplication<a class="headerlink" href="#task-3-4-cuda-matrix-multiplication" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with CUDA.
Be sure to read the Guide on
<a class="reference internal" href="cuda.html"><span class="doc">GPU Programming</span></a> and the Numba CUDA guide.</p>
</div>
<p>Finally we can combine both these approaches and implement CUDA
<cite>matmul</cite>. This operation is probably the most important in all of deep
learning and is central to making models fast. Again, we first strive for
accuracy, but, the faster you can make it, the better.</p>
<p>Implementing matrix multiplication and reduction efficiently is
hugely important for many deep learning tasks. Follow the guides provided
in class for implementing these functions.</p>
<p>You should document your code to show us that you
understand each line. Prove to us that these lead to speed-ups on
large matrix operations by making a graph comparing them to naive
operations.</p>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Implement  <cite>minitorch/cuda_ops.py</cite> with CUDA, and pass tests marked as <cite>task3_4</cite>. Follow the requirements
specified in the docs.</p>
</div>
<dl class="py function">
<dt id="minitorch.cuda_ops._mm_practice">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">_mm_practice</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out</span><span class="p">:</span> <span class="n">numpy.ndarray<span class="p">[</span>Any<span class="p">, </span>numpy.dtype<span class="p">[</span>numpy.float64<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">a</span><span class="p">:</span> <span class="n">numpy.ndarray<span class="p">[</span>Any<span class="p">, </span>numpy.dtype<span class="p">[</span>numpy.float64<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">b</span><span class="p">:</span> <span class="n">numpy.ndarray<span class="p">[</span>Any<span class="p">, </span>numpy.dtype<span class="p">[</span>numpy.float64<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">size</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#minitorch.cuda_ops._mm_practice" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a practice square MM kernel to prepare for matmul.</p>
<p>Given a storage <cite>out</cite> and two storage <cite>a</cite> and <cite>b</cite>. Where we know
both are shape [size, size] with strides [size, 1].</p>
<p>Size is always &lt; 32.</p>
<p>Requirements:</p>
<blockquote>
<div><ul class="simple">
<li><p>All data must be first moved to shared memory.</p></li>
<li><p>Only read each cell in <cite>a</cite> and <cite>b</cite> once.</p></li>
<li><p>Only write to global memory once per kernel.</p></li>
</ul>
</div></blockquote>
<p>Compute</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">:</span>
         <span class="k">for</span> <span class="n">k</span><span class="p">:</span>
             <span class="n">out</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>a</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>b</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>size</strong> (<em>int</em>) -- size of the square</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.cuda_ops.tensor_matrix_multiply">
<code class="sig-prename descclassname">minitorch.cuda_ops.</code><code class="sig-name descname">tensor_matrix_multiply</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.cuda_ops.tensor_matrix_multiply" title="Permalink to this definition">¶</a></dt>
<dd><p>CUDA Dispatcher object. When configured and called, the dispatcher will
specialize itself for the given arguments (if no suitable specialized
version already exists) &amp; compute capability, and launch on the device
associated with the current context.</p>
<p>Dispatcher objects are not to be constructed by the user, but instead are
created using the <code class="xref py py-func docutils literal notranslate"><span class="pre">numba.cuda.jit()</span></code> decorator.</p>
</dd></dl>

</section>
<section id="task-3-5-training">
<h3>Task 3.5: Training<a class="headerlink" href="#task-3-5-training" title="Permalink to this headline">¶</a></h3>
<p>If your code works, you should now be able to move on to the tensor
training script in <cite>project/run_fast_tensor.py</cite>.  This code is the same
basic training setup as  <a class="reference internal" href="module2.html"><span class="doc">Tensors</span></a>, but now utilizes your fast tensor
code. We have left the <cite>matmul</cite> layer blank for you to implement with
your tensor code.</p>
<div class="admonition-todo admonition" id="id5">
<p class="admonition-title">Todo</p>
<ul>
<li><p>Implement the missing functions in <cite>project/run_fast_tensor.py</cite>. These
should
do exactly the same thing as the corresponding functions in
<cite>project/run_tensor.py</cite>,
but now use the faster backend</p></li>
<li><p>Train a tensor model and add your results for all dataset
to the README.</p></li>
<li><p>Run a bigger model and record the time per epoch reported by the
trainer. Here is the command</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">run_fast_tensor</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">BACKEND</span> <span class="n">gpu</span> <span class="o">--</span><span class="n">HIDDEN</span> <span class="mi">100</span> <span class="o">--</span><span class="n">DATASET</span> <span class="n">split</span> <span class="o">--</span><span class="n">RATE</span> <span class="mf">0.05</span>
<span class="n">python</span> <span class="n">run_fast_tensor</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">BACKEND</span> <span class="n">cpu</span> <span class="o">--</span><span class="n">HIDDEN</span> <span class="mi">100</span> <span class="o">--</span><span class="n">DATASET</span> <span class="n">split</span> <span class="o">--</span><span class="n">RATE</span> <span class="mf">0.05</span>
</pre></div>
</div>
</li>
</ul>
<p>Train a tensor model and add your results for all three dataset
to the README. Also record the time per epoch reported by the
trainer. (As a reference, our parallel implementation gave a 10x speedup).
On a standard Colab GPU setup, aim for you CPU to get below 2 seconds per epoch and
GPU to be below 1 second per epoch. (With some cleverness you can do much better.)</p>
</div>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="tensor.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Tensor Variables</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="parallel.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Parallel Computation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
    <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, Sasha Rush.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>